{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'talk.religion.misc'])\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "vectors = np.asarray(vectorizer.fit_transform(train.data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Ensure numerical stability\n",
    "    exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_scores / exp_scores.sum(1)[:, np.newaxis]\n",
    "\n",
    "def crossentropy(x, y):\n",
    "    m = (x * y).sum(1)\n",
    "    ce = np.log(m)\n",
    "    return -ce.sum()\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, lr=0.1, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.random.uniform(low=-1., high=1., size=(X.shape[1], y.shape[1]))\n",
    "        self.b = np.random.uniform(low=-1., high=1., size=(y.shape[1]))\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # print(self.w)\n",
    "            z = X.dot(self.w) + self.b\n",
    "            z = softmax(z)\n",
    "            loss = crossentropy(z, y)\n",
    "\n",
    "            # Calculate gradients\n",
    "            dW = X.T.dot(z - y) / X.shape[0]\n",
    "            db = z.sum(0) / X.shape[0]\n",
    "\n",
    "            self.w = self.w - self.lr * dW\n",
    "            self.b = self.b - self.lr * db\n",
    "            if i % 10 == 0:\n",
    "                print(loss/X.shape[0])\n",
    "            # print(np.argmax(z, 1))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = X.dot(self.w) + self.b\n",
    "        z = softmax(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=-1., high=1., size=(5, 6))\n",
    "y = np.array([[0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = SoftmaxRegression(epochs=1000)\n",
    "# lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.argmax(lr.predict_proba(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "5.03546971365427\n",
      "3.4175330523841883\n",
      "2.647567189826352\n",
      "2.2184424578273405\n",
      "1.884372715524855\n",
      "1.6169406629514775\n",
      "1.4187645814541465\n",
      "1.260170453206355\n",
      "1.1277254359774234\n",
      "1.016027526664022\n",
      "0.9194384093852871\n",
      "0.8358569952135106\n",
      "0.7635409939769153\n",
      "0.6998854075131963\n",
      "0.6434473648088366\n",
      "0.5935493104605092\n",
      "0.5494311106007653\n",
      "0.5100912240026397\n",
      "0.4746010843041485\n",
      "0.4422383817523359\n",
      "0.4124519613214714\n",
      "0.38480666651597295\n",
      "0.3590246488944772\n",
      "0.33498486027959606\n",
      "0.3126579526821375\n",
      "0.2920092201114182\n",
      "0.2729483948120506\n",
      "0.25535680987583315\n",
      "0.23913349587851876\n",
      "0.22420937430871737\n",
      "0.21052842103313893\n",
      "0.19802648632527406\n",
      "0.18663504678301873\n",
      "0.1762964181934536\n",
      "0.16695900999795363\n",
      "0.15855453608030562\n",
      "0.15098748774526327\n",
      "0.14414535184739666\n",
      "0.13791718841417497\n",
      "0.1322079661891866\n",
      "0.12694277608181903\n",
      "0.12206429907582228\n",
      "0.1175284981101827\n",
      "0.11330074368095025\n",
      "0.10935275682460421\n",
      "0.105660294735684\n",
      "0.10220154443262844\n",
      "0.09895621162955405\n",
      "0.09590518996155685\n",
      "0.09303058437831363\n",
      "0.0903158532394947\n",
      "0.08774591810743258\n",
      "0.08530719023615409\n",
      "0.08298752378885746\n",
      "0.08077612395991048\n",
      "0.07866343371362168\n",
      "0.07664101355774708\n",
      "0.07470142214324181\n",
      "0.0728381021546704\n",
      "0.07104527441709675\n",
      "0.0693178421572404\n",
      "0.06765130642916839\n",
      "0.06604169282279512\n",
      "0.06448548882904075\n",
      "0.06297959071865515\n",
      "0.061521258511403366\n",
      "0.06010807752611847\n",
      "0.058737925048020945\n",
      "0.057408940770225576\n",
      "0.0561194998221718\n",
      "0.05486818737188695\n",
      "0.053653773985097644\n",
      "0.052475191159438155\n",
      "0.05133150674608711\n",
      "0.050221900332257464\n",
      "0.049145639068172\n",
      "0.048102054828033466\n",
      "0.047090523909269634\n",
      "0.046110450596334764\n",
      "0.04516125576102391\n",
      "0.044242371211922704\n",
      "0.04335323979113125\n",
      "0.0424933203722858\n",
      "0.04166209610982819\n",
      "0.04085908369309009\n",
      "0.04008384109501133\n",
      "0.039335971436383156\n",
      "0.038615121112427535\n",
      "0.03792097119613657\n",
      "0.03725322223968231\n",
      "0.03661157378583333\n",
      "0.03599570096801032\n",
      "0.0354052312862653\n",
      "0.0348397247968036\n",
      "0.03429866045810218\n",
      "0.03378143032339891\n",
      "0.033287341912195\n",
      "0.03281562777619381\n",
      "0.03236546030822728\n",
      "0.03193596939947118\n"
     ]
    }
   ],
   "source": [
    "# np.array([[1, 2], [3, 4]]) * np.array([[1, 2], [3, 4]])\n",
    "lr.fit(vectors, np.asarray(OneHotEncoder().fit_transform(train.target.reshape(len(train.target), 1)).todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1, 1, 0, 0, 1, 0, 0]), array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0]))"
      ]
     },
     "metadata": {},
     "execution_count": 374
    }
   ],
   "source": [
    "lr.predict_proba(vectors[:10]).argmax(1), train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.83719141],\n",
       "       [ 0.00482592],\n",
       "       [-0.01815983],\n",
       "       ...,\n",
       "       [-0.52647845],\n",
       "       [ 0.01647992],\n",
       "       [-0.72650787]])"
      ]
     },
     "metadata": {},
     "execution_count": 321
    }
   ],
   "source": [
    "lr.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "matrix([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 337
    }
   ],
   "source": [
    "# train.target.reshape(len(train.target), 1)\n",
    "OneHotEncoder().fit_transform(train.target.reshape(len(train.target), 1)).todense()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}