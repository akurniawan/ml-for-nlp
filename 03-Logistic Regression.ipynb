{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'talk.religion.misc'])\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "vectors = np.asarray(vectorizer.fit_transform(train.data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Ensure numerical stability\n",
    "    exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_scores / exp_scores.sum(1)[:, np.newaxis]\n",
    "\n",
    "def crossentropy(x, y):\n",
    "    ce = np.log((x * y).sum(1))\n",
    "    return -ce.sum()\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, lr=0.1, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.random.uniform(low=-1., high=1., size=(X.shape[1], y.shape[1]))\n",
    "        self.b = np.random.uniform(low=-1., high=1., size=(y.shape[1]))\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # print(self.w)\n",
    "            z = X.dot(self.w) + self.b\n",
    "            z = softmax(z)\n",
    "            loss = crossentropy(z, y)\n",
    "\n",
    "            # Calculate gradients\n",
    "            dW = X.T.dot(z - y) / X.shape[0]\n",
    "            db = z.sum(0) / X.shape[0]\n",
    "\n",
    "            self.w = self.w - self.lr * dW\n",
    "            self.b = self.b - self.lr * db\n",
    "            if i % 10 == 0:\n",
    "                print(loss/X.shape[0])\n",
    "            # print(np.argmax(z, 1))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = X.dot(self.w) + self.b\n",
    "        z = softmax(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=-1., high=1., size=(5, 6))\n",
    "y = np.array([[0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[0.22220086 0.19241954 0.15131995 0.04892512 0.16372816 0.22140637]\n [0.33497269 0.32903158 0.03410964 0.20551176 0.03646044 0.05991389]\n [0.28076857 0.44469807 0.0713154  0.11676161 0.06790137 0.01855498]\n [0.31102574 0.30325262 0.06207056 0.11841196 0.15803131 0.04720781]\n [0.19231823 0.16311174 0.20869488 0.04149031 0.30757542 0.08680941]]\n"
     ]
    }
   ],
   "source": [
    "lr = SoftmaxRegression(epochs=200)\n",
    "lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 319
    }
   ],
   "source": [
    "np.argmax(lr.predict_proba(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[[1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]\n [1.]]\n"
     ]
    }
   ],
   "source": [
    "lr.fit(vectors, train.target[:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([[1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.],\n",
       "        [1.]]),\n",
       " array([0, 0, 0, 1, 1]))"
      ]
     },
     "metadata": {},
     "execution_count": 324
    }
   ],
   "source": [
    "lr.predict_proba(vectors[:5]), train.target[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.83719141],\n",
       "       [ 0.00482592],\n",
       "       [-0.01815983],\n",
       "       ...,\n",
       "       [-0.52647845],\n",
       "       [ 0.01647992],\n",
       "       [-0.72650787]])"
      ]
     },
     "metadata": {},
     "execution_count": 321
    }
   ],
   "source": [
    "lr.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}