{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'talk.religion.misc'])\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "# vectors = np.asarray(vectorizer.fit_transform(train.data).todense())\n",
    "vectors = vectorizer.fit_transform(train.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    # Ensure numerical stability\n",
    "    exp_scores = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_scores / exp_scores.sum(1)[:, np.newaxis]\n",
    "\n",
    "def crossentropy(x, y):\n",
    "    m = (x * y).sum(1)\n",
    "    ce = np.log(m)\n",
    "    return -ce.sum()\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, lr=0.1, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.random.uniform(low=-1., high=1., size=(X.shape[1], y.shape[1]))\n",
    "        self.b = np.random.uniform(low=-1., high=1., size=(y.shape[1]))\n",
    "\n",
    "        for i in range(self.epochs):\n",
    "            # print(self.w)\n",
    "            z = X.dot(self.w) + self.b\n",
    "            z = softmax(z)\n",
    "            loss = crossentropy(z, y)\n",
    "\n",
    "            # Calculate gradients\n",
    "            dW = X.T.dot(z - y) / X.shape[0]\n",
    "            db = z.sum(0) / X.shape[0]\n",
    "\n",
    "            self.w = self.w - self.lr * dW\n",
    "            self.b = self.b - self.lr * db\n",
    "            if i % 10 == 0:\n",
    "                print(loss/X.shape[0])\n",
    "            # print(np.argmax(z, 1))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = X.dot(self.w) + self.b\n",
    "        z = softmax(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=-1., high=1., size=(5, 6))\n",
    "y = np.array([[0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lr = SoftmaxRegression(epochs=1000)\n",
    "# lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(857, 4098)"
      ]
     },
     "metadata": {},
     "execution_count": 6
    }
   ],
   "source": [
    "# np.argmax(lr.predict_proba(x), 1)\n",
    "vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "6.057801803676946\n",
      "4.151471683341053\n",
      "3.261882612689855\n",
      "2.692181900474255\n",
      "2.2838231696716647\n",
      "1.9679135813937165\n",
      "1.7075337138010267\n",
      "1.5077623759027632\n",
      "1.3368207747920917\n",
      "1.1889251891877424\n",
      "1.0629402303555529\n",
      "0.9567894973403738\n",
      "0.8636052710785648\n",
      "0.781585735094651\n",
      "0.711051799972983\n",
      "0.6499451830099303\n",
      "0.5969968458653672\n",
      "0.5498065767426916\n",
      "0.5067615130212798\n",
      "0.46720942033819435\n",
      "0.4309397123370505\n",
      "0.39815785882872556\n",
      "0.3683045124246737\n",
      "0.3407544707676108\n",
      "0.3153432869460376\n",
      "0.29201082353903984\n",
      "0.27068994900589005\n",
      "0.2513187523880089\n",
      "0.23382524093850332\n",
      "0.2180909519924279\n",
      "0.20393793191164114\n",
      "0.1911537119315527\n",
      "0.17953080599304566\n",
      "0.16889411864697926\n",
      "0.1591105457381788\n",
      "0.15008676979849755\n",
      "0.1417611943644535\n",
      "0.13409221646874778\n",
      "0.12704464701310278\n",
      "0.12057966985736476\n",
      "0.11465306047185037\n",
      "0.1092190283117479\n",
      "0.10423372649823219\n",
      "0.09965627158027851\n",
      "0.09544849674448003\n",
      "0.09157472098299808\n",
      "0.08800178444468763\n",
      "0.08469913590283581\n",
      "0.08163885501447873\n",
      "0.07879564385885393\n",
      "0.07614682216987449\n",
      "0.07367229949966542\n",
      "0.07135447969493879\n",
      "0.06917808136145175\n",
      "0.06712989148493254\n",
      "0.06519848514143814\n",
      "0.06337394336184148\n",
      "0.061647592550365476\n",
      "0.06001177896927499\n",
      "0.05845968357254448\n",
      "0.0569851766091663\n",
      "0.05558270777253526\n",
      "0.05424722583619629\n",
      "0.05297412121557902\n",
      "0.05175918526955839\n",
      "0.050598581005476546\n",
      "0.04948882087030112\n",
      "0.04842674829432231\n",
      "0.04740952049046767\n",
      "0.046434590666143194\n",
      "0.0454996882971046\n",
      "0.04460279650479019\n",
      "0.04374212594955291\n",
      "0.04291608507853417\n",
      "0.04212324709558394\n",
      "0.041362314644697216\n",
      "0.040632083841872474\n",
      "0.03993140981449858\n",
      "0.03925917614756187\n",
      "0.038614270462869395\n",
      "0.03799556774134078\n",
      "0.03740192204262896\n",
      "0.03683216619589892\n",
      "0.03628511808473952\n",
      "0.035759591531274795\n",
      "0.03525440958837774\n",
      "0.03476841823772008\n",
      "0.034300498940444075\n",
      "0.03384957904281131\n",
      "0.033414639569871805\n",
      "0.032994720363787554\n",
      "0.03258892280905702\n",
      "0.03219641054190591\n",
      "0.03181640859347273\n",
      "0.031448201399897055\n",
      "0.031091130057279523\n",
      "0.03074458912853542\n",
      "0.03040802323725932\n",
      "0.030080923619098126\n",
      "0.029762824747339357\n"
     ]
    }
   ],
   "source": [
    "# np.array([[1, 2], [3, 4]]) * np.array([[1, 2], [3, 4]])\n",
    "lr.fit(vectors, np.asarray(OneHotEncoder().fit_transform(train.target.reshape(len(train.target), 1)).todense()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 1, 1, 0, 0, 1, 0, 0]), array([0, 0, 0, 1, 1, 0, 0, 1, 0, 0]))"
      ]
     },
     "metadata": {},
     "execution_count": 374
    }
   ],
   "source": [
    "lr.predict_proba(vectors[:10]).argmax(1), train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[-0.83719141],\n",
       "       [ 0.00482592],\n",
       "       [-0.01815983],\n",
       "       ...,\n",
       "       [-0.52647845],\n",
       "       [ 0.01647992],\n",
       "       [-0.72650787]])"
      ]
     },
     "metadata": {},
     "execution_count": 321
    }
   ],
   "source": [
    "lr.w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "matrix([[1., 0.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.],\n",
       "        [0., 1.],\n",
       "        [1., 0.],\n",
       "        [1., 0.]])"
      ]
     },
     "metadata": {},
     "execution_count": 337
    }
   ],
   "source": [
    "# train.target.reshape(len(train.target), 1)\n",
    "OneHotEncoder().fit_transform(train.target.reshape(len(train.target), 1)).todense()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}