{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'talk.religion.misc'])\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "vectors = np.asarray(vectorizer.fit_transform(train.data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.exp(z).sum(1)[:, np.newaxis]\n",
    "\n",
    "def crossentropy(x, y):\n",
    "    ce = np.log((x * y).sum(1))\n",
    "    return -ce.sum()\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, lr=0.1, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.random.uniform(low=-1., high=1., size=(X.shape[1], y.shape[1]))\n",
    "        self.b = np.random.uniform(low=-1., high=1., size=(y.shape[1]))\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            # print(self.w)\n",
    "            z = X.dot(self.w) + self.b\n",
    "            z = softmax(z)\n",
    "            loss = crossentropy(z, y)\n",
    "\n",
    "            # Calculate gradients\n",
    "            dW = X.T.dot(z - y) / X.shape[0]\n",
    "            db = z.sum(0) / X.shape[0]\n",
    "\n",
    "            self.w = self.w - self.lr * dW\n",
    "            self.b = self.b - self.lr * db\n",
    "            print(loss/X.shape[0])\n",
    "            print(np.argmax(z, 1))\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        z = X.dot(self.w) + self.b\n",
    "        z = softmax(z)\n",
    "        return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=-1., high=1., size=(5, 6))\n",
    "y = np.array([[0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2.429505242892496\n[5 5 3 5 2]\n2.388285287814952\n[5 5 3 5 2]\n2.3481697336451846\n[5 5 3 5 2]\n2.3091130084688345\n[5 5 3 5 2]\n2.2710716801991024\n[5 5 3 5 2]\n2.2340044319300416\n[5 5 3 5 2]\n2.1978720200720794\n[5 5 3 5 2]\n2.162637219102411\n[5 5 3 5 2]\n2.1282647562048944\n[5 5 3 5 2]\n2.094721238558601\n[5 5 3 5 2]\n2.061975075570371\n[5 5 3 5 2]\n2.029996397937942\n[5 5 3 5 2]\n1.9987569750761136\n[5 5 3 5 2]\n1.9682301321358815\n[5 5 3 5 2]\n1.9383906675911184\n[5 5 1 5 2]\n1.909214772154104\n[5 5 1 5 2]\n1.880679949604609\n[5 5 1 5 2]\n1.852764939972251\n[5 5 1 5 2]\n1.8254496453936362\n[5 5 1 5 2]\n1.7987150588701304\n[5 5 1 5 2]\n1.772543196075135\n[5 5 1 5 2]\n1.7469170302982548\n[5 5 1 5 2]\n1.7218204305649514\n[5 5 1 5 2]\n1.6972381029317911\n[5 5 1 5 2]\n1.6731555349273386\n[5 5 1 5 2]\n1.6495589430854405\n[4 5 1 5 2]\n1.6264352234998132\n[4 5 1 5 2]\n1.6037719053153432\n[4 5 1 5 2]\n1.581557107061569\n[4 5 1 5 2]\n1.55977949572658\n[4 5 1 5 2]\n1.538428248464616\n[4 5 1 5 2]\n1.5174930168274214\n[4 5 1 5 2]\n1.4969638934076162\n[4 5 1 5 2]\n1.4768313807816267\n[4 5 1 5 2]\n1.4570863626399344\n[4 5 1 5 2]\n1.4377200769932945\n[4 5 1 5 2]\n1.4187240913450565\n[4 5 1 5 2]\n1.4000902797216512\n[4 5 1 5 2]\n1.3818108014556243\n[4 5 1 0 2]\n1.3638780816182041\n[4 5 1 0 2]\n1.3462847930012565\n[4 5 1 0 2]\n1.3290238395515483\n[4 5 1 0 2]\n1.3120883411634754\n[4 5 1 0 4]\n1.2954716197397789\n[1 5 1 0 4]\n1.27916718643326\n[1 5 1 0 4]\n1.2631687299860594\n[1 5 1 0 4]\n1.2474701060866988\n[1 5 1 0 4]\n1.2320653276687525\n[1 5 1 0 4]\n1.2169485560787\n[1 5 1 0 4]\n1.2021140930442131\n[1 5 1 0 4]\n1.1875563733778065\n[1 5 1 0 4]\n1.1732699583544288\n[1 5 1 0 4]\n1.1592495297051815\n[1 5 1 0 4]\n1.1454898841728867\n[1 5 1 0 4]\n1.1319859285786973\n[1 5 1 0 4]\n1.1187326753523177\n[1 5 1 0 4]\n1.1057252384816842\n[1 5 1 0 4]\n1.0929588298411275\n[1 5 1 0 4]\n1.0804287558600862\n[1 2 1 0 4]\n1.0681304144973722\n[1 2 1 0 4]\n1.0560592924887875\n[1 2 1 0 4]\n1.044210962838544\n[1 2 1 0 4]\n1.0325810825274835\n[1 2 1 0 4]\n1.0211653904134426\n[1 2 1 0 4]\n1.0099597053013984\n[1 2 1 0 4]\n0.9989599241630891\n[1 2 1 0 4]\n0.9881620204878082\n[1 2 1 0 4]\n0.9775620427478751\n[1 2 1 0 4]\n0.9671561129640024\n[1 2 1 0 4]\n0.9569404253573438\n[1 2 1 0 4]\n0.9469112450764581\n[1 2 1 0 4]\n0.9370649069887562\n[1 2 1 0 4]\n0.9273978145272144\n[1 2 1 0 4]\n0.9179064385842542\n[1 2 1 0 4]\n0.9085873164456935\n[1 2 1 0 4]\n0.8994370507585977\n[1 2 1 0 4]\n0.8904523085276811\n[1 2 1 0 4]\n0.8816298201356549\n[1 2 1 0 4]\n0.872966378383591\n[1 2 1 0 4]\n0.8644588375479584\n[1 2 1 0 4]\n0.8561041124515327\n[1 2 1 0 4]\n0.8478991775458322\n[1 2 1 0 4]\n0.8398410660031738\n[1 2 1 0 4]\n0.8319268688167801\n[1 2 1 0 4]\n0.8241537339077121\n[1 2 1 0 4]\n0.8165188652376656\n[1 2 1 0 4]\n0.8090195219269134\n[1 2 1 0 4]\n0.8016530173768859\n[1 2 1 0 4]\n0.794416718397055\n[1 2 1 0 4]\n0.7873080443359388\n[1 2 1 0 4]\n0.7803244662161654\n[1 2 1 0 4]\n0.773463505873646\n[1 2 1 0 4]\n0.7667227351009887\n[1 2 1 0 4]\n0.7600997747953482\n[1 2 1 0 4]\n0.7535922941109743\n[1 2 1 0 4]\n0.7471980096167483\n[1 2 1 0 4]\n0.7409146844590426\n[1 2 1 0 4]\n0.7347401275302412\n[1 2 1 0 4]\n0.7286721926432984\n[1 2 1 0 4]\n0.7227087777126873\n[1 2 1 0 4]\n0.7168478239421198\n[1 2 1 0 4]\n0.7110873150193943\n[1 2 1 0 4]\n0.7054252763187302\n[1 2 1 0 4]\n0.6998597741109308\n[1 2 1 0 4]\n0.6943889147817063\n[1 2 1 0 4]\n0.6890108440584642\n[1 2 1 0 4]\n0.6837237462458668\n[1 2 1 0 4]\n0.6785258434704259\n[1 2 1 0 4]\n0.6734153949343904\n[1 2 1 0 4]\n0.6683906961791596\n[1 2 1 0 4]\n0.6634500783584356\n[1 2 1 0 4]\n0.6585919075213031\n[1 2 1 0 4]\n0.6538145839054094\n[1 2 1 0 4]\n0.6491165412403919\n[1 2 1 0 4]\n0.6444962460616845\n[1 2 1 0 4]\n0.6399521970348104\n[1 2 1 0 4]\n0.6354829242902547\n[1 2 1 0 4]\n0.6310869887689858\n[1 2 1 0 4]\n0.6267629815786862\n[1 2 1 0 4]\n0.6225095233607258\n[1 2 1 0 4]\n0.618325263667904\n[1 2 1 0 4]\n0.6142088803529673\n[1 2 1 0 4]\n0.6101590789678959\n[1 2 1 0 4]\n0.6061745921739405\n[1 2 1 0 4]\n0.6022541791623768\n[1 2 1 0 4]\n0.5983966250859339\n[1 2 1 0 4]\n0.5946007405008423\n[1 2 1 0 4]\n0.590865360819436\n[1 2 1 0 4]\n0.5871893457732372\n[1 2 1 0 4]\n0.5835715788864375\n[1 2 1 0 4]\n0.5800109669596888\n[1 2 1 0 4]\n0.5765064395641019\n[1 2 1 0 4]\n0.5730569485453534\n[1 2 1 0 4]\n0.569661467537786\n[1 2 1 0 4]\n0.5663189914883886\n[1 2 1 0 4]\n0.5630285361905369\n[1 2 1 0 4]\n0.5597891378273652\n[1 2 1 0 4]\n0.5565998525246442\n[1 2 1 0 4]\n0.5534597559130325\n[1 2 1 0 4]\n0.5503679426995636\n[1 2 1 0 4]\n0.5473235262482319\n[1 2 1 0 4]\n0.5443256381695406\n[1 2 1 0 4]\n0.5413734279188648\n[1 2 1 0 4]\n0.5384660624034898\n[1 2 1 0 4]\n0.5356027255981782\n[1 2 1 0 4]\n0.5327826181691246\n[1 2 1 0 4]\n0.5300049571061437\n[1 2 1 0 4]\n0.5272689753629564\n[1 2 1 0 4]\n0.5245739215054188\n[1 2 1 0 4]\n0.5219190593675522\n[1 2 1 0 4]\n0.5193036677152274\n[1 2 1 0 4]\n0.5167270399173577\n[1 2 1 0 4]\n0.5141884836244563\n[1 2 1 0 4]\n0.5116873204544159\n[1 2 1 0 4]\n0.5092228856853636\n[1 2 1 0 4]\n0.5067945279554528\n[1 2 1 0 4]\n0.5044016089694547\n[1 2 1 0 4]\n0.5020435032120005\n[1 2 1 0 4]\n0.4997195976673484\n[1 2 1 0 4]\n0.497429291545531\n[1 2 1 0 4]\n0.49517199601475353\n[1 2 1 0 4]\n0.49294713393990686\n[1 2 1 0 4]\n0.4907541396270675\n[1 2 1 0 4]\n0.48859245857385314\n[1 2 1 0 4]\n0.48646154722550766\n[1 2 1 0 4]\n0.48436087273658934\n[1 2 1 0 4]\n0.48228991273814026\n[1 2 1 0 4]\n0.4802481551102148\n[1 2 1 0 4]\n0.4782350977596465\n[1 2 1 0 4]\n0.4762502484029385\n[1 2 1 0 4]\n0.47429312435415943\n[1 2 1 0 4]\n0.4723632523177338\n[1 2 1 0 4]\n0.4704601681860138\n[1 2 1 0 4]\n0.46858341684152355\n[1 2 1 0 4]\n0.4667325519637699\n[1 2 1 0 4]\n0.4649071358405129\n[1 2 1 0 4]\n0.46310673918339385\n[1 2 1 0 4]\n0.46133094094781874\n[1 2 1 0 4]\n0.4595793281570004\n[1 2 1 0 4]\n0.4578514957300581\n[1 2 1 0 4]\n0.4561470463140845\n[1 2 1 0 4]\n0.4544655901200823\n[1 2 1 0 4]\n0.45280674476268257\n[1 2 1 0 4]\n0.45117013510355397\n[1 2 1 0 4]\n0.4495553930984154\n[1 2 1 0 4]\n0.44796215764756797\n[1 2 1 0 4]\n0.44639007444986023\n[1 2 1 0 4]\n0.44483879586000896\n[1 2 1 0 4]\n0.4433079807491901\n[1 2 1 0 4]\n0.4417972943688264\n[1 2 1 0 4]\n0.4403064082174934\n[1 2 1 0 4]\n0.4388349999108684\n[1 2 1 0 4]\n0.43738275305465113\n[1 2 1 0 4]\n0.4359493571203833\n[1 2 1 0 4]\n0.4345345073240984\n[1 2 1 0 4]\n0.43313790450773276\n[1 2 1 0 4]\n0.4317592550232326\n[1 2 1 0 4]\n0.43039827061929037\n[1 2 1 0 4]\n0.4290546683306503\n[1 2 1 0 4]\n0.4277281703699156\n[1 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "lr = SoftmaxRegression(epochs=200)\n",
    "lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([1, 2, 1, 0, 4])"
      ]
     },
     "metadata": {},
     "execution_count": 292
    }
   ],
   "source": [
    "np.argmax(lr.predict_proba(x), 1)"
   ]
  }
 ]
}