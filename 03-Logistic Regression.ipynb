{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'talk.religion.misc'])\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "vectors = np.asarray(vectorizer.fit_transform(train.data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(z):\n",
    "    return np.exp(z) / np.exp(z).sum(1)[:, np.newaxis]\n",
    "\n",
    "def crossentropy(x, y):\n",
    "    ce = np.log((x * y).sum(1))\n",
    "    return -ce.sum()\n",
    "\n",
    "class SoftmaxRegression(object):\n",
    "    def __init__(self, lr=0.1, epochs=10):\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        self.w = np.random.uniform(low=-1., high=1., size=(X.shape[1], y.shape[1]))\n",
    "        self.b = np.random.uniform(low=-1., high=1., size=(y.shape[1]))\n",
    "\n",
    "        for _ in range(self.epochs):\n",
    "            # print(self.w)\n",
    "            z = X.dot(self.w) + self.b\n",
    "            z = softmax(z)\n",
    "            loss = crossentropy(z, y)\n",
    "\n",
    "            # Calculate gradients\n",
    "            dW = X.T.dot(z - y) / X.shape[0]\n",
    "            db = z.sum(0) / X.shape[0]\n",
    "\n",
    "            self.w = self.w - self.lr * dW\n",
    "            self.b = self.b - self.lr * db\n",
    "            print(loss/X.shape[0])\n",
    "            print(np.argmax(z, 1))\n",
    "\n",
    "    def predict(self, X):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.uniform(low=-1., high=1., size=(5, 6))\n",
    "y = np.array([[0, 1, 0, 0, 0, 0], [0, 0, 1, 0, 0, 0], [0, 1, 0, 0, 0, 0], [1, 0, 0, 0, 0, 0], [0, 0, 0, 0, 1, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.9699347503231415\n[1 5 5 5 5]\n1.9334417623534967\n[1 5 5 5 5]\n1.8979835304269848\n[1 5 5 5 5]\n1.863523493915006\n[1 5 5 5 5]\n1.8300263181252838\n[1 5 5 5 5]\n1.797457909445131\n[1 5 5 5 5]\n1.7657854225738845\n[1 5 5 5 5]\n1.7349772604175342\n[1 5 5 5 5]\n1.7050030672188292\n[1 5 5 5 5]\n1.675833715491743\n[1 5 5 5 5]\n1.647441287323597\n[1 5 1 5 5]\n1.6197990506033737\n[1 2 1 5 5]\n1.5928814307315293\n[1 2 1 5 5]\n1.5666639783644807\n[1 2 1 5 5]\n1.5411233337449315\n[1 2 1 5 5]\n1.5162371881658228\n[1 2 1 5 5]\n1.4919842431095987\n[1 2 1 5 2]\n1.4683441675943336\n[1 2 1 5 2]\n1.4452975542432083\n[1 2 1 5 2]\n1.4228258745732816\n[1 2 1 5 2]\n1.4009114339733455\n[1 2 1 5 2]\n1.3795373268091655\n[1 2 1 5 2]\n1.358687392058119\n[1 2 1 5 2]\n1.3383461698349763\n[1 2 1 5 2]\n1.3184988591272926\n[1 2 1 5 2]\n1.2991312770136734\n[1 2 1 5 2]\n1.2802298195920756\n[1 2 1 5 2]\n1.2617814247994363\n[1 2 1 5 2]\n1.2437735372591834\n[1 2 1 5 2]\n1.2261940752504943\n[1 2 1 5 2]\n1.2090313998532376\n[1 2 1 5 2]\n1.192274286285953\n[1 2 1 5 2]\n1.1759118974214395\n[1 2 1 5 2]\n1.1599337594358192\n[1 2 1 5 2]\n1.1443297395224818\n[1 2 1 5 2]\n1.1290900255821295\n[1 2 1 0 2]\n1.1142051077841082\n[1 2 1 0 2]\n1.099665761882224\n[1 2 1 0 2]\n1.0854630341599418\n[1 2 1 0 2]\n1.0715882278750197\n[1 2 1 0 2]\n1.058032891071821\n[1 2 1 0 2]\n1.0447888056304027\n[1 2 1 0 2]\n1.031847977424584\n[1 2 1 0 2]\n1.0192026274661825\n[1 2 1 0 2]\n1.0068451839190318\n[1 2 1 0 2]\n0.9947682748739446\n[1 2 1 0 2]\n0.9829647217840869\n[1 2 1 0 2]\n0.9714275334690068\n[1 2 1 0 2]\n0.9601499006045252\n[1 2 1 0 2]\n0.9491251906246386\n[1 2 1 0 2]\n0.9383469429703094\n[1 2 1 0 2]\n0.9278088646283779\n[1 2 1 0 2]\n0.9175048259117012\n[1 2 1 0 2]\n0.9074288564389406\n[1 2 1 0 2]\n0.8975751412791079\n[1 2 1 0 2]\n0.8879380172320154\n[1 2 1 0 2]\n0.8785119692211663\n[1 2 1 0 2]\n0.8692916267803469\n[1 2 1 0 2]\n0.8602717606192926\n[1 2 1 0 2]\n0.8514472792573125\n[1 2 1 0 2]\n0.8428132257167187\n[1 2 1 0 2]\n0.834364774270362\n[1 2 1 0 2]\n0.8260972272395761\n[1 2 1 0 2]\n0.8180060118404283\n[1 2 1 0 2]\n0.8100866770774064\n[1 2 1 0 2]\n0.802334890684608\n[1 2 1 0 2]\n0.7947464361151562\n[1 2 1 0 2]\n0.7873172095800172\n[1 2 1 0 4]\n0.7800432171376539\n[1 2 1 0 4]\n0.7729205718360692\n[1 2 1 0 4]\n0.7659454909087898\n[1 2 1 0 4]\n0.7591142930262512\n[1 2 1 0 4]\n0.752423395603893\n[1 2 1 0 4]\n0.7458693121680708\n[1 2 1 0 4]\n0.7394486497806566\n[1 2 1 0 4]\n0.7331581065229628\n[1 2 1 0 4]\n0.7269944690393697\n[1 2 1 0 4]\n0.7209546101407855\n[1 2 1 0 4]\n0.7150354864678482\n[1 2 1 0 4]\n0.7092341362135411\n[1 2 1 0 4]\n0.703547676904712\n[1 2 1 0 4]\n0.6979733032417901\n[1 2 1 0 4]\n0.6925082849958557\n[1 2 1 0 4]\n0.6871499649620716\n[1 2 1 0 4]\n0.6818957569683773\n[1 2 1 0 4]\n0.6767431439382594\n[1 2 1 0 4]\n0.6716896760063424\n[1 2 1 0 4]\n0.6667329686854846\n[1 2 1 0 4]\n0.6618707010840379\n[1 2 1 0 4]\n0.6571006141719062\n[1 2 1 0 4]\n0.65242050909403\n[1 2 1 0 4]\n0.6478282455299299\n[1 2 1 0 4]\n0.6433217400979597\n[1 2 1 0 4]\n0.6388989648029313\n[1 2 1 0 4]\n0.6345579455258201\n[1 2 1 0 4]\n0.6302967605542733\n[1 2 1 0 4]\n0.6261135391526989\n[1 2 1 0 4]\n0.6220064601707442\n[1 2 1 0 4]\n0.6179737506890177\n[1 2 1 0 4]\n0.6140136847009516\n[1 2 1 0 4]\n0.6101245818297506\n[1 2 1 0 4]\n0.6063048060794067\n[1 2 1 0 4]\n0.6025527646188136\n[1 2 1 0 4]\n0.5988669065980483\n[1 2 1 0 4]\n0.5952457219959301\n[1 2 1 0 4]\n0.5916877404980065\n[1 2 1 0 4]\n0.5881915304041516\n[1 2 1 0 4]\n0.5847556975649957\n[1 2 1 0 4]\n0.5813788843464356\n[1 2 1 0 4]\n0.578059768621514\n[1 2 1 0 4]\n0.5747970627889728\n[1 2 1 0 4]\n0.5715895128178182\n[1 2 1 0 4]\n0.56843589731726\n[1 2 1 0 4]\n0.5653350266314039\n[1 2 1 0 4]\n0.5622857419581068\n[1 2 1 0 4]\n0.5592869144914101\n[1 2 1 0 4]\n0.556337444586996\n[1 2 1 0 4]\n0.5534362609501209\n[1 2 1 0 4]\n0.550582319845496\n[1 2 1 0 4]\n0.547774604328598\n[1 2 1 0 4]\n0.5450121234979094\n[1 2 1 0 4]\n0.5422939117675933\n[1 2 1 0 4]\n0.5396190281601239\n[1 2 1 0 4]\n0.5369865556183967\n[1 2 1 0 4]\n0.5343956003368643\n[1 2 1 0 4]\n0.5318452911112365\n[1 2 1 0 4]\n0.5293347787063059\n[1 2 1 0 4]\n0.5268632352414568\n[1 2 1 0 4]\n0.524429853593435\n[1 2 1 0 4]\n0.5220338468159487\n[1 2 1 0 4]\n0.5196744475756926\n[1 2 1 0 4]\n0.5173509076043826\n[1 2 1 0 4]\n0.5150624971664042\n[1 2 1 0 4]\n0.5128085045416755\n[1 2 1 0 4]\n0.5105882355233414\n[1 2 1 0 4]\n0.5084010129299166\n[1 2 1 0 4]\n0.5062461761315006\n[1 2 1 0 4]\n0.5041230805897003\n[1 2 1 0 4]\n0.5020310974108975\n[1 2 1 0 4]\n0.49996961291250175\n[1 2 1 0 4]\n0.4979380282018505\n[1 2 1 0 4]\n0.49593575876740276\n[1 2 1 0 4]\n0.49396223408189827\n[1 2 1 0 4]\n0.4920168972171517\n[1 2 1 0 4]\n0.4900992044701595\n[1 2 1 0 4]\n0.48820862500020235\n[1 2 1 0 4]\n0.48634464047663606\n[1 2 1 0 4]\n0.4845067447370684\n[1 2 1 0 4]\n0.4826944434556231\n[1 2 1 0 4]\n0.48090725382100563\n[1 2 1 0 4]\n0.4791447042240858\n[1 2 1 0 4]\n0.47740633395472026\n[1 2 1 0 4]\n0.47569169290754576\n[1 2 1 0 4]\n0.47400034129648205\n[1 2 1 0 4]\n0.47233184937768236\n[1 2 1 0 4]\n0.4706857971806861\n[1 2 1 0 4]\n0.4690617742475264\n[1 2 1 0 4]\n0.46745937937955534\n[1 2 1 0 4]\n0.4658782203917542\n[1 2 1 0 4]\n0.4643179138743053\n[1 2 1 0 4]\n0.4627780849612012\n[1 2 1 0 4]\n0.46125836710568235\n[1 2 1 0 4]\n0.45975840186229117\n[1 2 1 0 4]\n0.4582778386753425\n[1 2 1 0 4]\n0.45681633467361343\n[1 2 1 0 4]\n0.4553735544710594\n[1 2 1 0 4]\n0.4539491699733743\n[1 2 1 0 4]\n0.4525428601902116\n[1 2 1 0 4]\n0.45115431105289083\n[1 2 1 0 4]\n0.4497832152374242\n[1 2 1 0 4]\n0.4484292719926909\n[1 2 1 0 4]\n0.44709218697360614\n[1 2 1 0 4]\n0.44577167207912477\n[1 2 1 0 4]\n0.44446744529493226\n[1 2 1 0 4]\n0.4431792305406742\n[1 2 1 0 4]\n0.4419067575215851\n[1 2 1 0 4]\n0.44064976158437785\n[1 2 1 0 4]\n0.43940798357726185\n[1 2 1 0 4]\n0.43818116971396026\n[1 2 1 0 4]\n0.4369690714416018\n[1 2 1 0 4]\n0.43577144531236744\n[1 2 1 0 4]\n0.4345880528587716\n[1 2 1 0 4]\n0.43341866047247024\n[1 2 1 0 4]\n0.4322630392864794\n[1 2 1 0 4]\n0.43112096506070313\n[1 2 1 0 4]\n0.4299922180706653\n[1 2 1 0 4]\n0.4288765829993471\n[1 2 1 0 4]\n0.4277738488320323\n[1 2 1 0 4]\n0.42668380875406947\n[1 2 1 0 4]\n0.42560626005145774\n[1 2 1 0 4]\n0.4245410040141729\n[1 2 1 0 4]\n0.4234878458421444\n[1 2 1 0 4]\n0.42244659455380623\n[1 2 1 0 4]\n0.42141706289713926\n[1 2 1 0 4]\n0.42039906726313037\n[1 2 1 0 4]\n0.4193924276015748\n[1 2 1 0 4]\n0.418396967339146\n[1 2 1 0 4]\n0.4174125132996716\n[1 2 1 0 4]\n0.41643889562653874\n[1 2 1 0 4]\n0.41547594770717355\n[1 2 1 0 4]\n"
     ]
    }
   ],
   "source": [
    "lr = SoftmaxRegression(epochs=200)\n",
    "lr.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([5, 7, 9])"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "np.array([[1, 2, 3], [4, 5, 6]]).sum(0) #- np.array([[1, 0, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "metadata": {},
     "execution_count": 146
    }
   ],
   "source": [
    "np.array([1, 2, 3]).dot(np.array([1, 2, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[0.07218434, 0.2013218 , 0.23452418, 0.07656565, 0.07224773,\n",
       "        0.3431563 ],\n",
       "       [0.0835608 , 0.12328371, 0.07127313, 0.17164807, 0.06835536,\n",
       "        0.48187892],\n",
       "       [0.07647032, 0.20040759, 0.1249502 , 0.1257835 , 0.2753365 ,\n",
       "        0.19705189],\n",
       "       [0.24958671, 0.09553723, 0.21197873, 0.24724828, 0.06897457,\n",
       "        0.12667447],\n",
       "       [0.33066292, 0.14937373, 0.11404556, 0.09154738, 0.15063097,\n",
       "        0.16373943]])"
      ]
     },
     "metadata": {},
     "execution_count": 201
    }
   ],
   "source": [
    "softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}