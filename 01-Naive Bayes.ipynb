{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38364bitmlfornlpconda8a01b745126c4bdcbb8300cda524a33d",
   "display_name": "Python 3.8.3 64-bit ('ml-for-nlp': conda)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from scipy.special import logsumexp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "train = fetch_20newsgroups(subset='train', categories=['alt.atheism', 'talk.religion.misc'])\n",
    "vectorizer = CountVectorizer(stop_words=\"english\", min_df=5)\n",
    "vectors = np.asarray(vectorizer.fit_transform(train.data).todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayesClassification(object):\n",
    "    def fit(self, X, y):\n",
    "        num_class = np.unique(y).shape[0]\n",
    "\n",
    "        self.prior = np.zeros((num_class))\n",
    "        for i in range(num_class):\n",
    "            self.prior[i] = (y == 1).sum() / y.shape[0]\n",
    "        self.log_prior = np.log(self.prior)\n",
    "\n",
    "        # Assumming the features are in the representation of bag-of-words\n",
    "        x_by_c = np.array([X[y == c] for c in range(num_class)]) + 1.0\n",
    "        sum_words = np.array([arr.sum(0) for arr in x_by_c])\n",
    "        total_words = sum_words.sum()\n",
    "        self.log_likelihood = np.log(sum_words / total_words)\n",
    "\n",
    "    def predict(self, X):\n",
    "        posterior = np.zeros((X.shape[0], self.prior.shape[0]))\n",
    "        for i, x in enumerate(X):\n",
    "            pos = x.astype(bool)\n",
    "            log_likelihood = self.log_likelihood[:, pos]\n",
    "            log_likelihood = log_likelihood.sum(1)\n",
    "            posterior[i] = self.log_prior + log_likelihood\n",
    "        proba = posterior - logsumexp(posterior, axis=1).reshape(-1, 1)\n",
    "        return np.exp(proba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = NaiveBayesClassification()\n",
    "c.fit(vectors, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(array([0.43990665, 0.43990665]),\n",
       " array([[-8.92654391, -8.8643373 , -8.90808185, ..., -8.93277446,\n",
       "         -8.91215517, -8.92654391],\n",
       "        [-9.16376119, -9.13020965, -9.15852557, ..., -9.15331723,\n",
       "         -9.17166637, -9.14555522]]))"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "c.prior, c.log_likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[9.99996866e-01, 3.13446690e-06],\n",
       "       [9.99999961e-01, 3.85015766e-08],\n",
       "       [1.00000000e+00, 2.41697085e-11],\n",
       "       ...,\n",
       "       [1.00000000e+00, 1.32435020e-18],\n",
       "       [9.99998718e-01, 1.28170010e-06],\n",
       "       [1.00000000e+00, 2.27917053e-11]])"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "c.predict(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bench = MultinomialNB().fit(vectors, train.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 6.72482499e-30],\n",
       "       [1.00000000e+00, 3.12984756e-10],\n",
       "       [1.00000000e+00, 1.10013597e-42],\n",
       "       ...,\n",
       "       [1.00000000e+00, 2.06943454e-68],\n",
       "       [1.00000000e+00, 2.07984869e-21],\n",
       "       [1.00000000e+00, 3.12470907e-66]])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "bench.predict_proba(vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}